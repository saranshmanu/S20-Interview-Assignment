{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.00001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/boston_housing.pickle','rb') as f:\n",
    "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0.\n",
    "dev[:, 3] = 1.\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([404, 13])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc4 = nn.Linear(24, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "james = sy.VirtualWorker(hook, id=\"james\")\n",
    "\n",
    "compute_nodes = [bob, alice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "            \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/51 (0%)]\tLoss: 694.438599\n",
      "Train Epoch: 1 [80/51 (20%)]\tLoss: 720.955017\n",
      "Train Epoch: 1 [160/51 (39%)]\tLoss: 728.808655\n",
      "Train Epoch: 1 [240/51 (59%)]\tLoss: 887.254028\n",
      "Train Epoch: 1 [320/51 (78%)]\tLoss: 457.007721\n",
      "Train Epoch: 1 [200/51 (98%)]\tLoss: 329.091431\n",
      "Train Epoch: 2 [0/51 (0%)]\tLoss: 692.727478\n",
      "Train Epoch: 2 [80/51 (20%)]\tLoss: 719.209351\n",
      "Train Epoch: 2 [160/51 (39%)]\tLoss: 727.093750\n",
      "Train Epoch: 2 [240/51 (59%)]\tLoss: 885.298889\n",
      "Train Epoch: 2 [320/51 (78%)]\tLoss: 455.514587\n",
      "Train Epoch: 2 [200/51 (98%)]\tLoss: 327.846405\n",
      "Train Epoch: 3 [0/51 (0%)]\tLoss: 691.022095\n",
      "Train Epoch: 3 [80/51 (20%)]\tLoss: 717.469299\n",
      "Train Epoch: 3 [160/51 (39%)]\tLoss: 725.386047\n",
      "Train Epoch: 3 [240/51 (59%)]\tLoss: 883.346619\n",
      "Train Epoch: 3 [320/51 (78%)]\tLoss: 454.032349\n",
      "Train Epoch: 3 [200/51 (98%)]\tLoss: 326.606903\n",
      "Train Epoch: 4 [0/51 (0%)]\tLoss: 689.318970\n",
      "Train Epoch: 4 [80/51 (20%)]\tLoss: 715.744141\n",
      "Train Epoch: 4 [160/51 (39%)]\tLoss: 723.682068\n",
      "Train Epoch: 4 [240/51 (59%)]\tLoss: 881.401367\n",
      "Train Epoch: 4 [320/51 (78%)]\tLoss: 452.545105\n",
      "Train Epoch: 4 [200/51 (98%)]\tLoss: 325.372986\n",
      "Train Epoch: 5 [0/51 (0%)]\tLoss: 687.613464\n",
      "Train Epoch: 5 [80/51 (20%)]\tLoss: 714.022522\n",
      "Train Epoch: 5 [160/51 (39%)]\tLoss: 721.977783\n",
      "Train Epoch: 5 [240/51 (59%)]\tLoss: 879.448975\n",
      "Train Epoch: 5 [320/51 (78%)]\tLoss: 451.049774\n",
      "Train Epoch: 5 [200/51 (98%)]\tLoss: 324.136261\n",
      "Train Epoch: 6 [0/51 (0%)]\tLoss: 685.900940\n",
      "Train Epoch: 6 [80/51 (20%)]\tLoss: 712.299866\n",
      "Train Epoch: 6 [160/51 (39%)]\tLoss: 720.266663\n",
      "Train Epoch: 6 [240/51 (59%)]\tLoss: 877.477600\n",
      "Train Epoch: 6 [320/51 (78%)]\tLoss: 449.541473\n",
      "Train Epoch: 6 [200/51 (98%)]\tLoss: 322.895325\n",
      "Train Epoch: 7 [0/51 (0%)]\tLoss: 684.183533\n",
      "Train Epoch: 7 [80/51 (20%)]\tLoss: 710.574280\n",
      "Train Epoch: 7 [160/51 (39%)]\tLoss: 718.549072\n",
      "Train Epoch: 7 [240/51 (59%)]\tLoss: 875.496460\n",
      "Train Epoch: 7 [320/51 (78%)]\tLoss: 448.026947\n",
      "Train Epoch: 7 [200/51 (98%)]\tLoss: 321.646027\n",
      "Train Epoch: 8 [0/51 (0%)]\tLoss: 682.455566\n",
      "Train Epoch: 8 [80/51 (20%)]\tLoss: 708.847656\n",
      "Train Epoch: 8 [160/51 (39%)]\tLoss: 716.817200\n",
      "Train Epoch: 8 [240/51 (59%)]\tLoss: 873.501587\n",
      "Train Epoch: 8 [320/51 (78%)]\tLoss: 446.502411\n",
      "Train Epoch: 8 [200/51 (98%)]\tLoss: 320.387756\n",
      "Train Epoch: 9 [0/51 (0%)]\tLoss: 680.714111\n",
      "Train Epoch: 9 [80/51 (20%)]\tLoss: 707.106506\n",
      "Train Epoch: 9 [160/51 (39%)]\tLoss: 715.062439\n",
      "Train Epoch: 9 [240/51 (59%)]\tLoss: 871.483704\n",
      "Train Epoch: 9 [320/51 (78%)]\tLoss: 444.962189\n",
      "Train Epoch: 9 [200/51 (98%)]\tLoss: 319.119141\n",
      "Train Epoch: 10 [0/51 (0%)]\tLoss: 678.962646\n",
      "Train Epoch: 10 [80/51 (20%)]\tLoss: 705.336304\n",
      "Train Epoch: 10 [160/51 (39%)]\tLoss: 713.285828\n",
      "Train Epoch: 10 [240/51 (59%)]\tLoss: 869.422363\n",
      "Train Epoch: 10 [320/51 (78%)]\tLoss: 443.403503\n",
      "Train Epoch: 10 [200/51 (98%)]\tLoss: 317.842346\n",
      "Total 9.96 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 602.1180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(), list())\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred.view(-1), target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train():\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        \n",
    "        # update remote models\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        # encrypted aggregation\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                copy_of_parameter = copy.copy(params[remote_index][param_i])\n",
    "#                 fixed_precision_param = copy_of_parameter.fix_prec()\n",
    "                encrypted_param = copy_of_parameter.share(bob, alice, crypto_provider=james)\n",
    "                param = encrypted_param.get()\n",
    "                spdz_params.append(param)\n",
    "#                 spdz_params.append(params[remote_index][param_i].copy().fix_precision().share(bob, alice, crypto_provider=james).get())\n",
    "            new_param = (spdz_params[0] + spdz_params[1]).get().float()/2\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        # cleanup\n",
    "        with torch.no_grad():\n",
    "            for model in params:\n",
    "                for param in model:\n",
    "                    param *= 0\n",
    "\n",
    "            for model in models:\n",
    "                model.get()\n",
    "\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[1].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 2\n",
      "Test set: Average loss: 615.8579\n",
      "\n",
      "Epoch 3\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 4\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 5\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 6\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 7\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 8\n",
      "Test set: Average loss: 615.8579\n",
      "\n",
      "Epoch 9\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Epoch 10\n",
      "Test set: Average loss: 615.8578\n",
      "\n",
      "Total 18.44 s\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
